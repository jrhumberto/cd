{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "008_NLP_sumario_mp3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+0lCwbQaqaGGElqxjDIqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrhumberto/cd/blob/main/008_NLP_sumario_mp3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fontes:\n",
        "\n",
        "- https://jlgjosue.medium.com/creating-a-podcast-of-book-summaries-and-articles-in-pdf-in-portuguese-with-ai-in-python-91dac01f862b"
      ],
      "metadata": {
        "id": "48t32EKGHDow"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wRRQLfwG-_n"
      },
      "outputs": [],
      "source": [
        "INPUT = ''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the Python library to read the PDF\n",
        "!pip install pypdf2import PyPDF2#URL where the book is located\n",
        "ULR_livro = './drive/MyDrive/machado_assis/pixarAvulsos.pdf'#leading the location indicated\n",
        "book = open (ULR_book, ‘rb’)#Reading the book\n",
        "pdfReader = PyPDF2.PdfFileReader (book)\n",
        "text = '' #var where all the text of the book will be#The tale is only between pages 3 and 32\n",
        "for num in range (3, 32):\n",
        " page = pdfReader.getPage (num)\n",
        " text = text + page.extractText ()"
      ],
      "metadata": {
        "id": "Y23NHs6OHKm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Package installation\n",
        "!pip install nltk #Installation of word dictionaries (corpus)\n",
        "!python -m nltk.downloader all #dividing our text into sentences and then into words\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentencas = sent_tokenize(text)\n",
        "palavras = word_tokenize(text.lower())#Removing the stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "stopwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]#Creating the frequency distribution\n",
        "from nltk.probability import FreqDist\n",
        "frequencia = FreqDist(palavras_sem_stopwords)#Separating the most important sentences\n",
        "from collections import defaultdict\n",
        "sentencas_importantes = defaultdict(int)#Loop to go through all the sentences and collect all the statistics\n",
        "for i, sentenca in enumerate(sentencas):\n",
        "    for palavra in word_tokenize(sentenca.lower()):\n",
        "        if palavra in frequencia:\n",
        "            sentencas_importantes[i] += frequencia[palavra]#\"n\" most important sentences\n",
        "from heapq import nlargest\n",
        "idx_sentencas_importantes = nlargest(4, sentencas_importantes, sentencas_importantes.get)# We have the summary! :)\n",
        "resumo = ''\n",
        "for i in sorted(idx_sentencas_importantes):\n",
        "    resumo = resumo + sentencas[i]"
      ],
      "metadata": {
        "id": "Rp1cvsWXHOK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "G2VIyvfyHOT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gtts import gTTSimport\n",
        "ostts = gTTS(resumo, lang='pt-br')\n",
        "tts.save('resumo.mp3')"
      ],
      "metadata": {
        "id": "8iBvvbeDHOYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mfdzd53mHOb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fontes:\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1H3i3VdLBIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQi2asZSLAUm",
        "outputId": "9aa1d04f-9403-469b-ef3f-23dc447baa1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8whlYvhNWKW",
        "outputId": "2069c6ff-dac5-4c72-f0b0-f2013626ace8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import Request, urlopen"
      ],
      "metadata": {
        "id": "tLjmBJC0Ng66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://ultimosegundo.ig.com.br/politica/2017-04-25/reforma-da-previdencia.html'\n",
        "link = Request(url,headers={'User-Agent': 'Mozilla/5.0'})\n",
        "pagina = urlopen(link).read().decode('utf-8', 'ignore')"
      ],
      "metadata": {
        "id": "VXbIv9MxNWOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(pagina, \"lxml\")\n",
        "texto = soup.find(id=\"noticia\").text"
      ],
      "metadata": {
        "id": "OVQV73VHNWTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "U9svCVRZNWW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas = sent_tokenize(texto)\n",
        "palavras = word_tokenize(texto.lower())"
      ],
      "metadata": {
        "id": "mFkI4bBlN5FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from string import punctuationstopwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]"
      ],
      "metadata": {
        "id": "KaDqCncpN5JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "frequencia = FreqDist(palavras_sem_stopwords)"
      ],
      "metadata": {
        "id": "1DCZaLjbOGpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "sentencas_importantes = defaultdict(int)"
      ],
      "metadata": {
        "id": "ZnGcqRVQOGt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentenca in enumerate(sentencas):\n",
        "    for palavra in word_tokenize(sentenca.lower()):\n",
        "        if palavra in frequencia:\n",
        "            sentencas_importantes[i] += frequencia[palavra]"
      ],
      "metadata": {
        "id": "GY3dgzWSOGyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest\n",
        "idx_sentencas_importantes = nlargest(4, sentencas_importantes, sentencas_importantes.get)"
      ],
      "metadata": {
        "id": "Dv7GSFy7NWbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(idx_sentencas_importantes):\n",
        "    print(sentencas[i])"
      ],
      "metadata": {
        "id": "TD03g5YzOY0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código Completo do Sumarizador:"
      ],
      "metadata": {
        "id": "vG349MtGOk60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import Request, urlopen\n",
        "\n",
        "link = Request('http://ultimosegundo.ig.com.br/politica/2017-04-25/reforma-da-previdencia.html',\n",
        "               headers={'User-Agent': 'Mozilla/5.0'})\n",
        "pagina = urlopen(link).read().decode('utf-8', 'ignore')\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(pagina, \"lxml\")\n",
        "texto = soup.find(id=\"noticia\").text\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentencas = sent_tokenize(texto)\n",
        "palavras = word_tokenize(texto.lower())\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "stopwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords]\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "frequencia = FreqDist(palavras_sem_stopwords)\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "sentencas_importantes = defaultdict(int)\n",
        "\n",
        "for i, sentenca in enumerate(sentencas):\n",
        "    for palavra in word_tokenize(sentenca.lower()):\n",
        "        if palavra in frequencia:\n",
        "            sentencas_importantes[i] += frequencia[palavra]\n",
        "\n",
        "from heapq import nlargest\n",
        "\n",
        "idx_sentencas_importantes = nlargest(4, sentencas_importantes, sentencas_importantes.get)\n",
        "\n",
        "for i in sorted(idx_sentencas_importantes):\n",
        "    print(sentencas[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXHNNqHSOY7R",
        "outputId": "5938da92-db81-4056-97b6-8745216d2dea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  \n",
            "\n",
            "\n",
            " Lúcio Bernardo Junior/Câmara dos Deputados - 19.4.17\n",
            "Deputados discutem na Comissão da Reforma da Previdência; com gravata roxa, o presidente do colegiado, Carlos Marun\n",
            "\n",
            "\n",
            "\n",
            "A comissão especial que analisa a proposta de reforma da Previdência na Câmara dos Deputados inicia na tarde desta terça-feira (25) a discussão do relatório apresentado na semana passada pelo relator\n",
            ", deputado Arthur Maia (PPS-BA).\n",
            "Depois de fechar acordo com parlamentares da oposição, que tentavam obstruir a sessão de leitura do parecer do relator, o presidente da comissão da reforma da Previdência\n",
            ", deputado Carlos Marun (PMDB-MS), designou que todas as reuniões desta semana sejam para discutir o relatório e apresentar pedido de vista.\n",
            "O relatório de Arthur Maia fixa a idade mínima de aposentadoria em 62 anos para as mulheres e em 65 anos para os homens após um período de transição de 20 anos.\n",
            "Para se tornar lei, a proposta de reforma da Previdência precisa, após ser aprovada na comissão especial, também passar por votação em dois turnos no plenário da Câmara e depois receber o aval do Senado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonte:\n",
        "- https://medium.com/@viniljf/criando-um-analisador-de-sentimentos-para-tweets-a53bae0c5147"
      ],
      "metadata": {
        "id": "NuvXrLRbK2uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "\n",
        "consumer_key = 'yAL0H3XVN3TFmZtSXxONNAuLJ'\n",
        "consumer_secret = '79OuTu1LvgmmiZERMKJ0pEHO6dmZx9M1ZU2AOi92x3clxUa14f'\n",
        "\n",
        "access_token = '54384307-XkH23t5V2scEApizDkjL1Dt9V7FRWnLryColjLX4j'\n",
        "access_token_secret = 'eHxk2mx6eNg5OcstPt8SslmCF6Gn56SfdQpCzMW70WUoD'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "auth.set_access_token(access_token,access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "#print(api.me())\n",
        "tweets = api.search('Emmanuel Macron')\n",
        "\n",
        "for tweet in tweets:\n",
        "    frase = TextBlob(tweet.text)\n",
        "\n",
        "    if frase.detect_language() != 'en':\n",
        "        traducao = TextBlob(str(frase.translate(to='en')))\n",
        "        print('Tweet: {0} - Sentimento: {1}'.format(tweet.text, traducao.sentiment))\n",
        "    else:\n",
        "        print('Tweet: {0} - Sentimento: {1}'.format(tweet.text, frase.sentiment))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "wctj5nE8HOfn",
        "outputId": "4d9f0c89-10e9-4cb3-e81c-2c181c3dd714"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TweepError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTweepError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-71ec02a62494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#print(api.me())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Emmanuel Macron'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tweepy/binder.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Parse the response payload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTweepError\u001b[0m: [{'code': 89, 'message': 'Invalid or expired token.'}]"
          ]
        }
      ]
    }
  ]
}