{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "003_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZj2AH4ffDEyT5A5tmfjwt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrhumberto/cd/blob/main/003_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fontes:\n",
        "- https://github.com/aasouzaconsult/Cientista-de-Dados/tree/master/NLP%20-%20Classifica%C3%A7%C3%A3o%20de%20Not%C3%ADcias%20Curtas%20PTB\n",
        "- https://medium.com/blog-do-zouza/classifica%C3%A7%C3%A3o-de-not%C3%ADcias-utilizando-machine-learning-b25ff63ea51f"
      ],
      "metadata": {
        "id": "WN0t7zMhnlR7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NMzrCabnh7M"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import pickle\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import FastText\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class E2V_AVG(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.w2v = word2vec\n",
        "        self.dimensao = 300\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        return self \n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "            np.mean([self.w2v[word] for word in words if word in self.w2v] or [np.zeros(self.dimensao)], axis=0)\n",
        "            for words in X\n",
        "        ])"
      ],
      "metadata": {
        "id": "UofhD3QEqfkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReferÊncia (SOUZA, 2019)\n",
        "class E2V_IDF(object):\n",
        "    def __init__(self, word2vec):\n",
        "        self.w2v = word2vec\n",
        "        self.wIDF = None # IDF da palavra na colecao\n",
        "        self.dimensao = 300\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
        "        tfidf.fit(X)\n",
        "        maximo_idf = max(tfidf.idf_) # Uma palavra que nunca foi vista (rara) então o IDF padrão é o máximo de idfs conhecidos (exemplo: 9.2525763918954524)\n",
        "        self.wIDF = defaultdict(\n",
        "            lambda: maximo_idf, \n",
        "            [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
        "        return self\n",
        "    \n",
        "    # Gera um vetor de 300 dimensões, para cada documento, com a média dos vetores (embeddings) dos termos * IDF, contidos no documento.\n",
        "    def transform(self, X):\n",
        "        return np.array([\n",
        "                np.mean([self.w2v[word] * self.wIDF[word] for word in words if word in self.w2v] or [np.zeros(self.dimensao)], axis=0)\n",
        "                for words in X\n",
        "            ])\n"
      ],
      "metadata": {
        "id": "1VT-ScVtqfoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arquivo com nóticias curtas em Português do site G1\n",
        "X = pickle.load(open('/data/z6News_X.ipy', 'rb'))\n",
        "# Arquivo com o rótulos das notícias\n",
        "y = pickle.load(open('/data/z6News_y.ipy', 'rb'))\n",
        "\n",
        "# Essa fonte de dados é própria e esta disponível aqui no GitHub na Pasta: data\n",
        "# - Podem utilizar, bastando referenciar o autor: SOUZA, 2019 (descrito na seção Referências)\n"
      ],
      "metadata": {
        "id": "c-WKZe2CqftH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Tranformando em Array\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n"
      ],
      "metadata": {
        "id": "px1dSdeiqfwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print (\"Total de Notícias - G1: %s\" % len(y))\n",
        "\n"
      ],
      "metadata": {
        "id": "aqAAHo4Cqf0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Word2Vec(X, size=300, window=5, sg=1, workers=4)\n",
        "w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.vectors)}\n",
        "\n"
      ],
      "metadata": {
        "id": "wcAxddBrqf4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5EndL7Qlqf71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pc84VF_DqgAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}