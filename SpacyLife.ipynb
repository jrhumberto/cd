{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyLife.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrhumberto/cd/blob/main/SpacyLife.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxlzWD-wYeFk"
      },
      "source": [
        "<b> Spacy Journey </b>\n",
        "\n",
        "1. Intro to NLP (Natural Language Processing)\n",
        "2. What is Spacy\n",
        "3. POS Tagging\n",
        "4. Stemming and Lemmatization\n",
        "5. Named entity recognition <a href = '#scrollTo=UO0TV-anfEgf'> [link] </a>\n",
        "6. Stop words\n",
        "7. Dependency Parsing\n",
        "8. Noun chunks\n",
        "9. Finding Similarity\n",
        "10. Glossary <a href = '#scrollTo=sICfJaxVl6qv'> [link] </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sICfJaxVl6qv"
      },
      "source": [
        "## Glossary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i39dheUYl0SX"
      },
      "source": [
        "| Item                           | Description                                                                                                        |\n",
        "|--------------------------------|--------------------------------------------------------------------------------------------------------------------|\n",
        "| Tokenization                   | Segmenting text into words, punctuation etc.                                                                       |\n",
        "| Lemmatization                  | Assigning the base forms of words, for example: \"was\" → \"be\" or \"rats\" → \"rat\".                                    |\n",
        "| Sentence Boundary Detection    | Finding and segmenting individual sentences.                                                                       |\n",
        "| Part-of-speech (POS) Tagging   | Assigning word types to tokens like verb or noun.                                                                  |\n",
        "| Dependency Parsing             | Assigning syntactic dependency labels, describing the relations between individual tokens, like subject or object. |\n",
        "| Named Entity Recognition (NER) | Labeling named \"real-world\" objects, like persons, companies or locations.                                         |\n",
        "| Text Classification            | Assigning categories or labels to a whole document, or parts of a document.                                        |\n",
        "| Statistical model              | Process for making predictions based on examples.                                                                  |\n",
        "| Training                       | Updating a statistical model with new examples.                                                                    |\n",
        "| Similarity                       | Comparing words, text spans and documents and how similar they are to each other.                                     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRua58gRhtn6"
      },
      "source": [
        "# Short Introduction to Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjY2m6J_ZYug"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4LY5nBoZhUW",
        "outputId": "34c2c421-13da-4a32-e619-5809a984e887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
            "Sebastian Thrun PERSON\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Thrun ORG\n",
            "Recode PRODUCT\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO0TV-anfEgf"
      },
      "source": [
        "\n",
        "## Named Entity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMmrCmnYbVYW",
        "outputId": "ed804342-9fb2-4fa2-b807-3605763f56e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCwpljimhj1a"
      },
      "source": [
        "\n",
        "|    TEXT    | START | END | LABEL |                     DESCRIPTION                      |\n",
        "|------------|-------|-----|-------|------------------------------------------------------|\n",
        "| Apple      |     0 |   5 | ORG   | Companies, agencies, institutions.                   |\n",
        "| U.K.       |    27 |  31 | GPE   | Geopolitical entity, i.e. countries, cities, states. |\n",
        "| $1 billion |    44 |  54 | MONEY | Monetary values, including unit.                     |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIZJkIfxh3DT"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdhvyUX4g5hK",
        "outputId": "c800f4d1-5fce-4df7-ec54-24d60eff6efd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple Apple PROPN NNP\n",
            "is be AUX VBZ\n",
            "looking look VERB VBG\n",
            "at at ADP IN\n",
            "buying buy VERB VBG\n",
            "U.K. U.K. PROPN NNP\n",
            "startup startup NOUN NN\n",
            "for for ADP IN\n",
            "$ $ SYM $\n",
            "1 1 NUM CD\n",
            "billion billion NUM CD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX3-Pl02j467"
      },
      "source": [
        "| TEXT    | LEMMA   | POS   | TAG |\n",
        "|---------|---------|-------|-----|\n",
        "| Apple   | apple   | PROPN | NNP |\n",
        "| is      | be      | VERB  | VBZ |\n",
        "| looking | look    | VERB  | VBG |\n",
        "| at      | at      | ADP   | IN  |\n",
        "| buying  | buy     | VERB  | VBG |\n",
        "| U.K.    | u.k.    | PROPN | NNP |\n",
        "| startup | startup | NOUN  | NN  |\n",
        "| for     | for     | ADP   | IN  |\n",
        "| $       | $       | SYM   | $   |\n",
        "| 1       | 1       | NUM   | CD  |\n",
        "| billion | billion | NUM   | CD  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZwKeXwYmvnQ"
      },
      "source": [
        "## Stop Words removal using Spacy\n",
        "\n",
        "*Let's Stop the STOP WORDS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVg4pi6kiKd-",
        "outputId": "9e8c4520-cce3-429a-c686-7103dfb7f0eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "\n",
        "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
        "\n",
        "#  \"nlp\" Object is used to create documents with linguistic annotations.\n",
        "my_doc = nlp(text)\n",
        "\n",
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Create list of word tokens after removing stopwords\n",
        "filtered_sentence =[] \n",
        "\n",
        "for word in token_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "        filtered_sentence.append(word) \n",
        "print(token_list)\n",
        "print(filtered_sentence)   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
            "['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ts1I8Rdnv56"
      },
      "source": [
        "## Stemming VS Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avwn0Wbbn0gK"
      },
      "source": [
        "Both are Text Normalisation Techniques used for reducing Machine Learing Modelling Time. \n",
        "\n",
        "<b>Stemming</b> algorithm works by cutting the suffix or prefix from the word. Lemmatization is a more powerful operation as it takes into consideration the morphological analysis of the word.\n",
        "\n",
        "<b> Lemmatization </b> returns the lemma, which is the root word of all its inflection forms.\n",
        "\n",
        "We can say that stemming is a quick and dirty method of chopping off words to its root form while on the other hand, lemmatization is an intelligent operation that uses dictionaries which are created by in-depth linguistic knowledge. Hence, Lemmatization helps in forming better features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR2RWZqAnDWC",
        "outputId": "b21a62cc-dfa0-458c-9867-f109842c612f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "#make sure to download the english model with \"python -m spacy download en\"\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
        "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
        "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
        "\n",
        "lemma_word1 = [] \n",
        "for token in doc:\n",
        "    lemma_word1.append(token.lemma_)\n",
        "print(lemma_word1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['-PRON-', 'determine', 'to', 'drop', '-PRON-', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', '-PRON-', 'claim', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', '-PRON-', 'be', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'right', 'have', 'become', 'much', 'less', 'valuable', ',', 'and', '-PRON-', 'have', '\\n', 'indeed', 'the', 'vague', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'be', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZJA4h0ZorK7"
      },
      "source": [
        "`NOTE: Stemming is not available in Spacy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sGookhduJ-e"
      },
      "source": [
        "## Noun Chunks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvPzh9H5pBA7",
        "outputId": "490f4a37-f126-45d4-cd1a-06069b391816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Generate Noun Phrases \n",
        "doc = nlp(u'I love data science on analytics vidhya') \n",
        "for np in doc.noun_chunks:\n",
        "    print (np.text, np.root.dep_, np.root.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I nsubj love\n",
            "data science dobj love\n",
            "analytics pobj on\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrLiwoSLxXeq"
      },
      "source": [
        "Dependency Parsing ([link](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkq115QV25D-"
      },
      "source": [
        "## Finding Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU9W4GXAzU4n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "ecf0a53a-372b-4900-ed51-7f3e028a6f88"
      },
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz --no-deps"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-md==2.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.0/en_core_web_md-2.2.0.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.0-cp36-none-any.whl size=98072934 sha256=dedf3bebb2f71f26fa2e53d05e654d03261c4d4cd8384b5bab1423531504e7c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/3e/c9/36dd6e13b449fd84cd1f94b72dfbc559daf09f53dbf4e697a3\n",
            "Successfully built en-core-web-md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbJ-K3AN1TYX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139a1eb1-00b5-4566-e908-fd26af91975e"
      },
      "source": [
        "spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f5bab15ea90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgCpMAJhuHGe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f8ab244e-c0a8-4ce0-c86f-f03ea4ea25b4"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_md\n",
        "#nlp = spacy.load(\"en_core_web_md\")\n",
        "nlp = en_core_web_md.load()\n",
        "tokens = nlp(\"dog cat banana afskfsd\")\n",
        "\n",
        "for token in tokens:\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog True 7.0336733 False\n",
            "cat True 6.6808186 False\n",
            "banana True 6.700014 False\n",
            "afskfsd False 0.0 True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM5GwCfay7Nv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "bbd794ab-da45-408e-f36c-2680f763d825"
      },
      "source": [
        "nlp = en_core_web_md.load() # make sure to use larger model!\n",
        "tokens = nlp(\"dog cat banana\")\n",
        "\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog dog 1.0\n",
            "dog cat 0.80168545\n",
            "dog banana 0.24327643\n",
            "cat dog 0.80168545\n",
            "cat cat 1.0\n",
            "cat banana 0.28154364\n",
            "banana dog 0.24327643\n",
            "banana cat 0.28154364\n",
            "banana banana 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqW9sXTz00eU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}