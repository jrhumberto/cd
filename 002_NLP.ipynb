{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "002_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1BMqvV/Ll0qZNmQM4f8m3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jrhumberto/cd/blob/main/002_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fontes:\n",
        "- https://www.linkedin.com/pulse/classifica%C3%A7%C3%A3o-de-textos-em-python-luiz-felipe-araujo-nunes/?originalSubdomain=pt\n",
        "- https://github.com/luizfan/nlp"
      ],
      "metadata": {
        "id": "a0ql9UuqRKti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQtFqlmiRJXl",
        "outputId": "5bd9cbbb-af97-4a1c-f78f-84b48210568c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk \n",
        "from nltk.stem import RSLPStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp') #Stemming\n",
        "nltk.download('punkt') #Tokenizacao"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Tokenize(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = nltk.word_tokenize(sentence)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "3RzWUpSkSJ5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase = Tokenize(\"Eu gosto de correr\")\n",
        "print(frase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5MXrK_ESgAR",
        "outputId": "52392a3b-b8c5-4547-d3bb-e371c15af529"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gosto', 'de', 'correr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Stemming(sentence):\n",
        "    stemmer = RSLPStemmer()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(stemmer.stem(word.lower()))\n",
        "    return phrase"
      ],
      "metadata": {
        "id": "tMxGx7tgSgKz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase1 = Tokenize(\"Eu gosto de correr\")\n",
        "frase2 = Tokenize(\"Eu gosto de corrida\")\n",
        "frase1 = Stemming(frase1)\n",
        "frase2 = Stemming(frase2)\n",
        "print(frase1)\n",
        "print(frase2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wBk3SR6SgRa",
        "outputId": "c94859a5-84c4-4b0d-c205-4e5b5e5cc175"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eu', 'gost', 'de', 'corr']\n",
            "['eu', 'gost', 'de', 'corr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def RemoveStopWords(sentence):\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        if word not in stopwords:\n",
        "            phrase.append(word)\n",
        "    return phrase"
      ],
      "metadata": {
        "id": "f2zQ9W-XSKHD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frase1 = RemoveStopWords(frase1)\n",
        "frase2 = RemoveStopWords(frase2)\n",
        "print(frase1)\n",
        "print(frase2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68YFlUxLSKRo",
        "outputId": "63be0cec-a4a0-4693-ba6d-a818917f7bc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gost', 'corr']\n",
            "['gost', 'corr']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Base de exemplos\n",
        "def Train():\n",
        "    training_data = []\n",
        "    training_data.append({\"classe\":\"amor\", \"frase\":\"Eu te amo\"})\n",
        "    training_data.append({\"classe\":\"amor\", \"frase\":\"Você é o amor da minha vida\"})\n",
        "    training_data.append({\"classe\":\"medo\", \"frase\":\"estou com medo\"})\n",
        "    training_data.append({\"classe\":\"medo\", \"frase\":\"tenho medo de fantasma\"})\n",
        "    print(\"%s frases incluidas\" % len(training_data))\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "didDo2TRRXqV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados = Train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3vL9JyrSCR1",
        "outputId": "6ca00dd0-ab35-41ae-8dde-b8fdf8734ec3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 frases incluidas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Learning(training_data):\n",
        "    corpus_words = {}\n",
        "    for data in training_data:\n",
        "        frase = data['frase']\n",
        "        frase = Tokenize(frase)\n",
        "        frase = Stemming(frase)\n",
        "        frase = RemoveStopWords(frase)\n",
        "        class_name = data['classe']\n",
        "        if class_name not in list(corpus_words.keys()):\n",
        "            corpus_words[class_name] = {}\n",
        "        for word in frase:\n",
        "            if word not in list(corpus_words[class_name].keys()):\n",
        "                corpus_words[class_name][word] = 1\n",
        "            else:\n",
        "                corpus_words[class_name][word] += 1\n",
        "    return corpus_words"
      ],
      "metadata": {
        "id": "v__h0tsJRXx-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados = Learning(dados)\n",
        "print(dados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uupe7jpifotR",
        "outputId": "4286142e-0415-48f4-9fa3-8b32a913be12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'amor': {'amo': 1, 'voc': 1, 'am': 1, 'minh': 1, 'vid': 1}, 'medo': {'est': 1, 'med': 2, 'tenh': 1, 'fantasm': 1}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_score(sentence,class_name):\n",
        "    score = 0 \n",
        "    sentence = Tokenize(sentence)\n",
        "    sentence = Stemming(sentence)\n",
        "    for word in sentence:\n",
        "        if word in dados[class_name]:\n",
        "            score += dados[class_name][word]\n",
        "    return score"
      ],
      "metadata": {
        "id": "terwLqjhRX4g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_class_score(\"tenho medo de baratas\",\"medo\")\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpboI07mf140",
        "outputId": "4759befe-c618-42d0-dd65-a22a5b4ae1e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_score(sentence):\n",
        "    high_score = 0\n",
        "    classname = 'default'\n",
        "    for classe in dados.keys():\n",
        "        pontos = 0\n",
        "        pontos = calculate_class_score(sentence,classe)\n",
        "        if pontos > high_score:\n",
        "            high_score = pontos\n",
        "            classname = classe\n",
        "    return classname,high_score"
      ],
      "metadata": {
        "id": "1gIkW2diRcqH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(calculate_score(\"eu amo aquela casa\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfYZ9XYNgEkV",
        "outputId": "3f561c64-c933-406c-8b11-2e13fab86282"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('amor', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "import nltk \n",
        "import re\n",
        "import yaml\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "expression = '[!-@[-`{-¿ÆÐÑ×ØÝ-ßä-æëðñö-øý-ÿ]'\n",
        "\n",
        "#Carrega o Corpus Words\n",
        "def LoadMemory():\n",
        "    fileW = open(\"words.nlp\", 'r')\n",
        "    words = fileW.read()\n",
        "    fileW.close()\n",
        "    words = yaml.load(words)\n",
        "    return words\n",
        "\n",
        "#Carrega as frases que foram treinadas\n",
        "def LoadExamples():\n",
        "    fileE = open(\"examples.nlp\", 'r')\n",
        "    examples = fileE.read()\n",
        "    fileE.close()\n",
        "    return examples\n",
        "\n",
        "#Salva a corpus words\n",
        "def SaveMemory(w):\n",
        "    fileW = open(\"words.nlp\", 'w')\n",
        "    print(\"Salvando...\")\n",
        "    fileW.write(str(w))\n",
        "    fileW.close()\n",
        "\n",
        "#Salva as novas frases treinadas\n",
        "def SaveExample(example):\n",
        "    fileE = open(\"examples.nlp\", 'a')\n",
        "    fileE.write(example + \"\\n\")\n",
        "    fileE.close()\n",
        "\n",
        "#Massa de dados para exemplo\n",
        "def Examples():\n",
        "    training_data = []\n",
        "    training_data.append({\"class\":\"saudade\", \"sentence\":\"sinto sua falta\"})\n",
        "    training_data.append({\"class\":\"saudade\", \"sentence\":\"estou com saudades\"})\n",
        "\n",
        "    training_data.append({\"class\":\"fome\", \"sentence\":\"estou com fome\"})\n",
        "    training_data.append({\"class\":\"fome\", \"sentence\":\"to faminto\"})\n",
        "\n",
        "    training_data.append({\"class\":\"medo\", \"sentence\":\"to com medo\"})\n",
        "    training_data.append({\"class\":\"medo\", \"sentence\":\"tomei um susto\"})\n",
        "\n",
        "    Learning(training_data)\n",
        "\n",
        "#Função responsavel por treinar a frase\n",
        "def Learning(training_data):\n",
        "    corpus_words = LoadMemory()\n",
        "\n",
        "    for data in training_data:\n",
        "        examples = LoadExamples()\n",
        "        sentence = data['sentence']\n",
        "        sentence = re.sub(expression, '', sentence)\n",
        "        sentence = stemmer.stem(sentence.lower())\n",
        "\n",
        "\n",
        "        if sentence in examples:\n",
        "            continue\n",
        "        \n",
        "        SaveExample(sentence)\n",
        "        sentence = nltk.word_tokenize(sentence)\n",
        "        class_name = data['class']\n",
        "        if class_name not in list(corpus_words.keys()):\n",
        "            corpus_words[class_name] = {}\n",
        "        for word in sentence:\n",
        "            if word not in list(corpus_words[class_name].keys()):\n",
        "                corpus_words[class_name][word] = 1\n",
        "            else:\n",
        "                corpus_words[class_name][word] += 1\n",
        "\n",
        "    \n",
        "SaveMemory(corpus_words)"
      ],
      "metadata": {
        "id": "nI9kPlNCRc1O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classification.py\n",
        "\n",
        "\n",
        "\n",
        "import nltk \n",
        "import re\n",
        "import yaml\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "expression = '[!-@[-`{-¿ÆÐÑ×ØÝ-ßä-æëðñö-øý-ÿ]'\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "\n",
        "#Carrega o Corpus Words\n",
        "def LoadMemory():\n",
        "    fileW = open(\"words.nlp\", 'r')\n",
        "    words = fileW.read()\n",
        "    words = yaml.load(words)\n",
        "    return words\n",
        "\n",
        "#Função responsavel por calcular a pontuação por classe\n",
        "def calculate_class_score(sentence,class_name):\n",
        "    score = 0\n",
        "    sentence = re.sub(expression, '', sentence)\n",
        "    sentence = nltk.word_tokenize(sentence)\n",
        "    for word in sentence:\n",
        "        if stemmer.stem(word.lower()) in corpus_words[class_name]:\n",
        "            score += corpus_words[class_name][stemmer.stem(word.lower())]\n",
        "    return score\n",
        "\n",
        "#Função responsavel por classificar a frase\n",
        "def classifique(sentence):\n",
        "    high_class = None\n",
        "    high_score = 0\n",
        "    for c in list(corpus_words.keys()):\n",
        "        score = calculate_class_score(sentence, c)\n",
        "        if score > high_score:\n",
        "            high_class = c\n",
        "            high_score = score\n",
        "\n",
        "    print(str(high_class))\n",
        "    return high_class\n",
        "\n",
        "memory = LoadMemory()\n",
        "corpus_words = memory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "u8J-p4jLRdBK",
        "outputId": "e2e59911-e6b8-42d0-d54d-4e00c2a90ff9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f1dddbad692f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhigh_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mcorpus_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f1dddbad692f>\u001b[0m in \u001b[0;36mLoadMemory\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Carrega o Corpus Words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoadMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfileW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words.nlp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfileW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'words.nlp'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquivos Finais: utils.py, train.py, classify.py, server.py, calculator.py, answer.py\n",
        "\n",
        "Files de textos:\n",
        "- https://raw.githubusercontent.com/luizfan/nlp/master/text/answer.txt\n",
        "\n",
        "- https://raw.githubusercontent.com/luizfan/nlp/master/text/corpus.txt\n",
        "\n",
        "- https://raw.githubusercontent.com/luizfan/nlp/master/text/stopwords.txt"
      ],
      "metadata": {
        "id": "17424pIoicoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# answer.py\n",
        "from utils import save_answer,load_answer\n",
        "\n",
        "def return_answer(class_name):\n",
        "    answers = load_answer()\n",
        "    try:\n",
        "        return answers[class_name]\n",
        "    except:\n",
        "        return 'Não entendi :('\n",
        "\n",
        "def include_answer(class_name,answer):\n",
        "    answers = load_answer()\n",
        "    answers[class_name] = answer\n",
        "    save_answer(answers)"
      ],
      "metadata": {
        "id": "z-QrsHREjI-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculator.py\n",
        "from utils import normalize,stemming,remove_stopwords,load_corpus\n",
        "\n",
        "def calculate_class_score(sentence,class_name):\n",
        "    score = 0 \n",
        "    sentence = normalize(sentence)\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    sentence = stemming(sentence)\n",
        "    dados = load_corpus()\n",
        "    for word in sentence:\n",
        "        if word in dados[class_name]:\n",
        "            score += dados[class_name][word]\n",
        "    return score\n",
        "\n",
        "def calculate_score(sentence):\n",
        "    high_score = 0\n",
        "    classname = 'default'\n",
        "    dados = load_corpus()\n",
        "    for classe in dados.keys():\n",
        "        pontos = 0\n",
        "        pontos = calculate_class_score(sentence,classe)\n",
        "        if pontos > high_score:\n",
        "            high_score = pontos\n",
        "            classname = classe\n",
        "    return {'classname':classname,'high_score':high_score}"
      ],
      "metadata": {
        "id": "Ck_AYb7BjI0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py\n",
        "from utils import normalize,stemming,remove_stopwords,load_corpus\n",
        "\n",
        "def learning(training_data):\n",
        "    corpus_words = load_corpus()\n",
        "    for data in training_data:\n",
        "        phrase = data['phrase']\n",
        "        phrase = normalize(phrase)\n",
        "        phrase = remove_stopwords(phrase)\n",
        "        phrase = stemming(phrase)\n",
        "\n",
        "        class_name = data['class']\n",
        "        if class_name not in list(corpus_words.keys()):\n",
        "            corpus_words[class_name] = {}\n",
        "        for word in phrase:\n",
        "            if word not in list(corpus_words[class_name].keys()):\n",
        "                corpus_words[class_name][word] = 1\n",
        "            else:\n",
        "                corpus_words[class_name][word] += 1\n",
        "    return corpus_words\n",
        "\n",
        "\n",
        "def sample():\n",
        "    training_data = []\n",
        "    training_data.append({\"class\":\"amor\", \"phrase\":\"Eu te amo\"})\n",
        "    training_data.append({\"class\":\"amor\", \"phrase\":\"Você é o amor da minha vida\"})\n",
        "    training_data.append({\"class\":\"medo\", \"phrase\":\"estou com medo\"})\n",
        "    training_data.append({\"class\":\"medo\", \"phrase\":\"tenho medo de fantasma\"})\n",
        "    training_data.append({\"class\":\"fome\", \"phrase\":\"eu estou com fome\"})\n",
        "    training_data.append({\"class\":\"fome\", \"phrase\":\"estou faminto\"})\n",
        "    print(\"%s phrases included\" % len(training_data))\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "xIlOBu0PlA1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.py\n",
        "import yaml\n",
        "import nltk\n",
        "import unicode\n",
        "import unicodedata\n",
        "import codecs\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "def normalize(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = strip_accents(sentence)\n",
        "    sentence = nltk.word_tokenize(sentence)\n",
        "    return sentence\n",
        "\n",
        "def strip_accents(sentence):\n",
        "    try:\n",
        "        sentence = unicode(sentence, 'utf-8')\n",
        "    except NameError: # unicode is a default on python 3 \n",
        "        pass\n",
        "\n",
        "    sentence = unicodedata.normalize('NFD', sentence)\\\n",
        "           .encode('ascii', 'ignore')\\\n",
        "           .decode(\"utf-8\")\n",
        "    return str(sentence)\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    stopwords = load_stopword()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        if word not in stopwords:\n",
        "            phrase.append(word)\n",
        "    return phrase\n",
        "\n",
        "def stemming(sentence):\n",
        "    stemmer = RSLPStemmer()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(stemmer.stem(word.lower()))\n",
        "    return phrase\n",
        "\n",
        "def save_corpus(w):\n",
        "    fileW = codecs.open(\"text/corpus.txt\", \"w\", \"utf-8\")\n",
        "    fileW.write(str(w))\n",
        "    fileW.close()\n",
        "\n",
        "def load_corpus():\n",
        "    with open(\"text/corpus.txt\", \"rb\") as f:\n",
        "        words = f.read().decode(\"UTF-8\")\n",
        "    words = yaml.load(words,Loader=yaml.FullLoader)\n",
        "    if words is None:\n",
        "        return {}\n",
        "    return words\n",
        "\n",
        "def load_stopword():\n",
        "    with open('text/stopwords.txt', 'r') as f:\n",
        "        stopwords = [strip_accents(line.strip()) for line in f] \n",
        "    return stopwords\n",
        "\n",
        "def save_answer(w):\n",
        "    fileW = codecs.open(\"text/answer.txt\", \"w\", \"utf-8\")\n",
        "    fileW.write(str(w))\n",
        "    fileW.close()\n",
        "\n",
        "def load_answer():\n",
        "    with open(\"text/answer.txt\", \"rb\") as f:\n",
        "        answer = f.read().decode(\"UTF-8\")\n",
        "    answer = yaml.load(answer,Loader=yaml.FullLoader)\n",
        "    if answer is None:\n",
        "        return {}\n",
        "    return answer"
      ],
      "metadata": {
        "id": "1-28nNc4iScG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# server.py\n",
        "'''\n",
        "from flask import Flask,json,request\n",
        "from train import learning,sample\n",
        "from utils import save_corpus,normalize,remove_stopwords,stemming\n",
        "from answer import return_answer,include_answer\n",
        "from calculator import calculate_score\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\", methods = ['GET'])\n",
        "def health_check():\n",
        "    return create_response(200,{\"status\":\"UP\"})\n",
        "\n",
        "@app.route(\"/train\", methods = ['GET'])\n",
        "def train_with_examples():\n",
        "    save_corpus(learning(sample()))\n",
        "    return create_response(200,{\"status\":\"sample phrases included\"})\n",
        "\n",
        "@app.route(\"/train\", methods = ['POST'])\n",
        "def train():\n",
        "    phrase = request.form.get('phrase')\n",
        "    class_name = request.form.get('class')\n",
        "    save_corpus(learning([{'class':class_name,'phrase':phrase}]))\n",
        "    return create_response(200,{\"status\":\"phrase included\"})\n",
        "\n",
        "@app.route(\"/classify\", methods = ['GET'])\n",
        "def classify():\n",
        "    phrase = request.form.get('phrase')\n",
        "    return create_response(200,calculate_score(phrase))\n",
        "\n",
        "@app.route(\"/chat\", methods = ['GET'])\n",
        "def chat():\n",
        "    phrase = request.form.get('phrase')\n",
        "    return create_response(200,{'answer':return_answer(calculate_score(phrase)['classname'])})\n",
        "\n",
        "@app.route(\"/answer\", methods = ['POST'])\n",
        "def save_answer():\n",
        "    answer = request.form.get('answer')\n",
        "    classname = request.form.get('class')\n",
        "    include_answer(classname,answer)\n",
        "    return create_response(200,{\"status\":\"answer included\"})\n",
        "\n",
        "@app.route(\"/normalize\", methods = ['GET'])\n",
        "def return_normalize():\n",
        "    phrase = request.form.get('phrase')\n",
        "    return create_response(200,{\"phrase\":normalize(phrase)})\n",
        "\n",
        "@app.route(\"/stopwords\", methods = ['GET'])\n",
        "def return_remove_stopwords():\n",
        "    phrase = request.form.get('phrase')\n",
        "    return create_response(200,{\"phrase\":remove_stopwords(normalize(phrase))})\n",
        "\n",
        "@app.route(\"/stemming\", methods = ['GET'])\n",
        "def return_stemming():\n",
        "    phrase = request.form.get('phrase')\n",
        "    return create_response(200,{\"phrase\":stemming(remove_stopwords(normalize(phrase)))})\n",
        "\n",
        "def create_response(statusCode, data):\n",
        "    response = app.response_class(\n",
        "        response=json.dumps(data),\n",
        "        status=statusCode,\n",
        "        mimetype='application/json'\n",
        "    )\n",
        "    return response\n",
        "\n",
        "app.run(host='127.0.0.1', port=8081)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "4myt7JJ4jIAQ",
        "outputId": "201075c9-487e-4f19-d0d6-6e248258d3ec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom flask import Flask,json,request\\nfrom train import learning,sample\\nfrom utils import save_corpus,normalize,remove_stopwords,stemming\\nfrom answer import return_answer,include_answer\\nfrom calculator import calculate_score\\n\\napp = Flask(__name__)\\n\\n@app.route(\"/\", methods = [\\'GET\\'])\\ndef health_check():\\n    return create_response(200,{\"status\":\"UP\"})\\n\\n@app.route(\"/train\", methods = [\\'GET\\'])\\ndef train_with_examples():\\n    save_corpus(learning(sample()))\\n    return create_response(200,{\"status\":\"sample phrases included\"})\\n\\n@app.route(\"/train\", methods = [\\'POST\\'])\\ndef train():\\n    phrase = request.form.get(\\'phrase\\')\\n    class_name = request.form.get(\\'class\\')\\n    save_corpus(learning([{\\'class\\':class_name,\\'phrase\\':phrase}]))\\n    return create_response(200,{\"status\":\"phrase included\"})\\n\\n@app.route(\"/classify\", methods = [\\'GET\\'])\\ndef classify():\\n    phrase = request.form.get(\\'phrase\\')\\n    return create_response(200,calculate_score(phrase))\\n\\n@app.route(\"/chat\", methods = [\\'GET\\'])\\ndef chat():\\n    phrase = request.form.get(\\'phrase\\')\\n    return create_response(200,{\\'answer\\':return_answer(calculate_score(phrase)[\\'classname\\'])})\\n\\n@app.route(\"/answer\", methods = [\\'POST\\'])\\ndef save_answer():\\n    answer = request.form.get(\\'answer\\')\\n    classname = request.form.get(\\'class\\')\\n    include_answer(classname,answer)\\n    return create_response(200,{\"status\":\"answer included\"})\\n\\n@app.route(\"/normalize\", methods = [\\'GET\\'])\\ndef return_normalize():\\n    phrase = request.form.get(\\'phrase\\')\\n    return create_response(200,{\"phrase\":normalize(phrase)})\\n\\n@app.route(\"/stopwords\", methods = [\\'GET\\'])\\ndef return_remove_stopwords():\\n    phrase = request.form.get(\\'phrase\\')\\n    return create_response(200,{\"phrase\":remove_stopwords(normalize(phrase))})\\n\\n@app.route(\"/stemming\", methods = [\\'GET\\'])\\ndef return_stemming():\\n    phrase = request.form.get(\\'phrase\\')\\n    return create_response(200,{\"phrase\":stemming(remove_stopwords(normalize(phrase)))})\\n\\ndef create_response(statusCode, data):\\n    response = app.response_class(\\n        response=json.dumps(data),\\n        status=statusCode,\\n        mimetype=\\'application/json\\'\\n    )\\n    return response\\n\\napp.run(host=\\'127.0.0.1\\', port=8081)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}